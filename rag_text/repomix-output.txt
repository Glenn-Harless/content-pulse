This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-21T22:03:16.696Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
app/
  db/
    database.py
    models.py
  static/
    app.js
    index.html
    styles.css
  __init__.py
  llm.py
  main.py
  scraper.py
migrations/
  versions/
    initial_migration.py
  env.py
.gitignore
alembic.ini
docker-compose.yml
Dockerfile
entrypoint.sh
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: app/db/database.py
================
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
import os

# Get database URL from environment variable
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://contentpulse:contentpulse@db:5432/contentpulse")

# Create async engine
engine = create_async_engine(
    DATABASE_URL,
    echo=False,  # Set to True for SQL logging
    pool_pre_ping=True,  # Enable connection health checks
    pool_size=5,  # Adjust based on your needs
    max_overflow=10
)

# Create async session factory
AsyncSessionLocal = sessionmaker(
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# Create base class for declarative models
Base = declarative_base()

async def init_db():
    """Initialize database tables"""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

================
File: app/db/models.py
================
from sqlalchemy import Column, Integer, String, DateTime, Text, Enum as SQLEnum, Index
from sqlalchemy.dialects.postgresql import JSONB
from datetime import datetime
import enum
from .database import Base

class ContentType(enum.Enum):
    ARTICLE = "article"
    REDDIT = "reddit"
    TWEET = "tweet"

class Content(Base):
    __tablename__ = 'content'

    id = Column(Integer, primary_key=True)
    type = Column(SQLEnum(ContentType), nullable=False)
    url = Column(String(512), unique=True, nullable=False)
    title = Column(String(512), nullable=False)
    content = Column(Text, nullable=False)
    summary = Column(Text, nullable=True)
    source = Column(String(100), nullable=False)
    scraped_at = Column(DateTime(timezone=True), nullable=False, index=True)
    extra_data = Column(JSONB, nullable=True, default={})

    # Add composite index for common queries
    __table_args__ = (
        Index('idx_type_scraped_at', type, scraped_at),
    )

    def to_dict(self):
        """Convert model instance to dictionary"""
        return {
            "id": self.id,
            "type": self.type.value,
            "url": self.url,
            "title": self.title,
            "content": self.content,
            "summary": self.summary,
            "source": self.source,
            "scraped_at": self.scraped_at.isoformat(),
            "extra_data": self.extra_data or {}
        }

================
File: app/static/app.js
================
document.addEventListener('DOMContentLoaded', () => {
    const scrapeBtn = document.getElementById('scrapeBtn');
    const queryBtn = document.getElementById('queryBtn');
    const articlesContainer = document.getElementById('articles');
    const queryInput = document.getElementById('queryInput');
    const queryResult = document.getElementById('queryResult');

    // Setup WebSocket connection
    const ws = new WebSocket(`ws://${window.location.host}/ws`);
    
    ws.onmessage = function(event) {
        const update = JSON.parse(event.data);
        updateArticleSummary(update.articleId, update.summary);
    };

    // Load articles on page load
    fetchArticles();

    scrapeBtn.addEventListener('click', async () => {
        setLoading(scrapeBtn, true);
        try {
            console.log('Starting scrape request...');
            const response = await fetch('/scrape', {
                method: 'POST',
                headers: {
                    'Accept': 'application/json',
                    'Content-Type': 'application/json'
                }
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const data = await response.json();
            console.log('Scrape response:', data);
            await fetchArticles();
        } catch (error) {
            console.error('Error scraping articles:', error);
            alert('Error scraping articles. Please try again.');
        } finally {
            setLoading(scrapeBtn, false);
        }
    });

    queryBtn.addEventListener('click', async () => {
        const question = queryInput.value.trim();
        if (!question) return;

        setLoading(queryBtn, true);
        try {
            console.log('Sending query:', question);
            const response = await fetch('/query', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Accept': 'application/json'
                },
                body: JSON.stringify({ question })
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const data = await response.json();
            queryResult.textContent = data.response;
        } catch (error) {
            console.error('Error querying articles:', error);
            alert('Error querying articles. Please try again.');
        } finally {
            setLoading(queryBtn, false);
        }
    });

    async function fetchArticles() {
        try {
            console.log('Fetching articles...');
            const response = await fetch('/articles', {
                headers: {
                    'Accept': 'application/json'
                }
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const articles = await response.json();
            console.log('Fetched articles:', articles);
            displayArticles(articles);
        } catch (error) {
            console.error('Error fetching articles:', error);
            articlesContainer.innerHTML = '<div class="alert alert-danger">Error loading articles</div>';
        }
    }

    function displayArticles(articles) {
        articlesContainer.innerHTML = articles.length ? '' : 
            '<div class="alert alert-info">No articles available. Click "Scrape Latest Articles" to fetch some!</div>';
        
        articles.forEach(article => {
            const articleElement = document.createElement('div');
            articleElement.className = 'card article-card';
            articleElement.innerHTML = `
                <div class="card-body">
                    <h5 class="card-title">${escapeHtml(article.title)}</h5>
                    <p class="timestamp">Scraped: ${new Date(article.scraped_at).toLocaleString()}</p>
                    <div class="summary-section" id="summary-${article.id}">
                        ${article.summary ? 
                            `<p class="article-summary">${escapeHtml(article.summary)}</p>` :
                            `<div class="summary-loading">
                                <div class="d-flex align-items-center">
                                    <div class="spinner-border spinner-border-sm me-2"></div>
                                    <span>Generating summary...</span>
                                </div>
                                <div class="progress-bar-container mt-2">
                                    <div class="progress-bar" style="width: 0%"></div>
                                </div>
                            </div>`
                        }
                    </div>
                    <div class="mt-3">
                        <a href="${article.url}" target="_blank" class="btn btn-outline-primary btn-sm me-2">Read Full Article</a>
                        ${!article.summary ? 
                            `<button class="btn btn-outline-secondary btn-sm generate-summary" data-article-id="${article.id}">
                                Generate Summary
                            </button>` : ''
                        }
                    </div>
                </div>
            `;
            articlesContainer.appendChild(articleElement);

            // Add event listener for generate summary button if it exists
            const generateBtn = articleElement.querySelector('.generate-summary');
            if (generateBtn) {
                generateBtn.addEventListener('click', () => generateSummary(article.id));
            }
        });
    }

    async function generateSummary(articleId) {
        try {
            const response = await fetch(`/generate-summary/${articleId}`, {
                method: 'POST',
                headers: {
                    'Accept': 'application/json',
                    'Content-Type': 'application/json'
                }
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            // Summary generation started - UI will update via WebSocket
            const summarySection = document.getElementById(`summary-${articleId}`);
            summarySection.innerHTML = `
                <div class="summary-loading">
                    <div class="d-flex align-items-center">
                        <div class="spinner-border spinner-border-sm me-2"></div>
                        <span>Generating summary...</span>
                    </div>
                    <div class="progress-bar-container mt-2">
                        <div class="progress-bar" style="width: 0%"></div>
                    </div>
                </div>
            `;
        } catch (error) {
            console.error('Error generating summary:', error);
            alert('Error generating summary. Please try again.');
        }
    }

    function updateArticleSummary(articleId, summary, progress = 100) {
        const summarySection = document.getElementById(`summary-${articleId}`);
        if (!summarySection) return;

        if (progress < 100) {
            // Update progress bar
            const progressBar = summarySection.querySelector('.progress-bar');
            if (progressBar) {
                progressBar.style.width = `${progress}%`;
            }
        } else {
            // Summary is complete
            summarySection.innerHTML = `<p class="article-summary">${escapeHtml(summary)}</p>`;
            
            // Remove the generate summary button if it exists
            const generateBtn = document.querySelector(`button[data-article-id="${articleId}"]`);
            if (generateBtn) {
                generateBtn.remove();
            }
        }
    }

    function setLoading(button, isLoading) {
        const spinner = button.querySelector('.spinner-border');
        spinner.classList.toggle('d-none', !isLoading);
        button.disabled = isLoading;
    }

    function escapeHtml(unsafe) {
        return unsafe
            .replace(/&/g, "&amp;")
            .replace(/</g, "&lt;")
            .replace(/>/g, "&gt;")
            .replace(/"/g, "&quot;")
            .replace(/'/g, "&#039;");
    }
});

================
File: app/static/index.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blockworks Article Analyzer</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/static/styles.css" rel="stylesheet">
</head>
<body>
    <nav class="navbar navbar-light bg-light">
        <div class="container">
            <span class="navbar-brand mb-0 h1">Blockworks Article Analyzer</span>
        </div>
    </nav>

    <div class="container mt-4">
        <div class="row mb-4">
            <div class="col">
                <button id="scrapeBtn" class="btn btn-primary">
                    <span class="spinner-border spinner-border-sm d-none" role="status" aria-hidden="true"></span>
                    Scrape Latest Articles
                </button>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8">
                <div id="articles" class="articles-container">
                </div>
            </div>

            <div class="col-md-4">
                <div class="card">
                    <div class="card-body">
                        <h5 class="card-title">Ask about Articles</h5>
                        <div class="mb-3">
                            <textarea id="queryInput" class="form-control" rows="3" placeholder="Ask a question about the articles..."></textarea>
                        </div>
                        <button id="queryBtn" class="btn btn-primary">
                            <span class="spinner-border spinner-border-sm d-none" role="status" aria-hidden="true"></span>
                            Ask Question
                        </button>
                        <div id="queryResult" class="mt-3"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="/static/app.js"></script>
</body>
</html>

================
File: app/static/styles.css
================
.articles-container {
    max-height: 80vh;
    overflow-y: auto;
}

.article-card {
    margin-bottom: 1rem;
    border-left: 4px solid #007bff;
}

.article-summary {
    font-style: italic;
    color: #666;
}

.loading {
    opacity: 0.5;
    pointer-events: none;
}

#queryResult {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 0.25rem;
    margin-top: 1rem;
    white-space: pre-wrap;
}

.timestamp {
    font-size: 0.8rem;
    color: #666;
}

.summary-loading {
    padding: 1rem;
    background: #f8f9fa;
    border-radius: 0.25rem;
}

.progress-bar-container {
    width: 100%;
    height: 4px;
    background: #f0f0f0;
    border-radius: 2px;
    overflow: hidden;
    margin-top: 0.5rem;
}

.progress-bar {
    height: 100%;
    background: #007bff;
    transition: width 0.3s ease;
}

.me-2 {
    margin-right: 0.5rem;
}

.mt-2 {
    margin-top: 0.5rem;
}

.mt-3 {
    margin-top: 1rem;
}

================
File: app/__init__.py
================


================
File: app/llm.py
================
"""
This script provides a simple interface to llama.ai's text generation model. It allows for
generating summaries of articles and answering questions based on a list of articles.
"""
import httpx
import json
from typing import Dict, List, Optional
import os
import logging
import asyncio
from typing import AsyncGenerator

logger = logging.getLogger(__name__)

class LlamaInterface:
    def __init__(self):
        self.base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        
    async def _generate_streaming_response(self, prompt: str, model: str = "llama3.2") -> str:
        """Generate response with streaming to avoid timeouts"""
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": True,
                        "options": {
                            "temperature": 0.3,
                            "top_k": 40,
                            "top_p": 0.9,
                            "num_predict": 200,
                        }
                    },
                    headers={"Accept": "application/x-ndjson"},
                )
                
                if response.status_code != 200:
                    return f"Error: API returned status code {response.status_code}"

                # Collect the streamed responses
                full_response = ""
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    try:
                        chunk = json.loads(line)
                        if chunk.get("response"):
                            full_response += chunk["response"]
                        if chunk.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue

                return full_response.strip()
                
        except httpx.TimeoutException as e:
            logger.error(f"Request timed out: {str(e)}")
            return "Error: Request timed out while generating response"
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            return f"Error generating response: {str(e)}"

    async def generate_summary(self, text: str, model: Optional[str] = None) -> str:
        """Generate article summary using specified or default model"""
        # Truncate very long articles to prevent context length issues
        max_chars = 4000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = f"""Please provide a concise 2-3 sentence summary of the following article:
        
        Article: {text}
        
        Summary:"""
        
        return await self._generate_streaming_response(prompt, model or "llama3.2")

    async def query_articles(self, articles: List[Dict], query: str, model: Optional[str] = None) -> str:
        """Query articles using specified or default model"""
        # Prepare context, but limit length
        context_parts = []
        total_chars = 0
        max_chars_per_article = 2000
        max_total_chars = 6000
        
        for article in articles:
            content = article['content']
            if len(content) > max_chars_per_article:
                content = content[:max_chars_per_article] + "..."
                
            article_text = f"Article: {article['title']}\n{content}\n"
            if total_chars + len(article_text) > max_total_chars:
                break
                
            context_parts.append(article_text)
            total_chars += len(article_text)
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""Based on these articles, please answer this question: {query}

        Context:
        {context}

        Answer:"""
        
        return await self._generate_streaming_response(prompt, model or "llama3.2")

================
File: app/main.py
================
from fastapi import FastAPI, HTTPException, WebSocket, Depends
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Set
import asyncio
import json
from datetime import datetime, timedelta
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from .db.models import Content, ContentType
from .db.database import AsyncSessionLocal
from .scraper import BlockworksScraper
from .llm import LlamaInterface
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Content Pulse")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

scraper = BlockworksScraper()
llm = LlamaInterface()

# Mount static files
app.mount("/static", StaticFiles(directory="app/static"), name="static")

# Store active WebSocket connections
active_connections: Set[WebSocket] = set()

class Query(BaseModel):
    question: str
    model: Optional[str] = None

class ModelSelection(BaseModel):
    model: str

# Dependency for database session
async def get_db():
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connections for real-time updates"""
    await websocket.accept()
    active_connections.add(websocket)
    try:
        while True:
            await websocket.receive_text()  # Keep connection alive
    except Exception as e:
        logger.error(f"WebSocket error: {str(e)}")
    finally:
        active_connections.remove(websocket)

async def notify_clients(article_id: str, summary: str, progress: int = 100):
    """Notify all connected clients of a summary update"""
    message = json.dumps({
        "articleId": article_id,
        "summary": summary,
        "progress": progress
    })
    for connection in active_connections:
        try:
            await connection.send_text(message)
        except Exception as e:
            logger.error(f"Error sending WebSocket message: {str(e)}")
            active_connections.remove(connection)

@app.get("/")
async def read_root():
    return FileResponse("app/static/index.html")

@app.post("/scrape")
async def scrape_articles(
    limit: int = 5,
    model: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    try:
        # Get recent articles to avoid re-scraping
        query = select(Content).where(
            Content.type == ContentType.ARTICLE,
            Content.scraped_at >= datetime.utcnow() - timedelta(hours=24)
        ).order_by(Content.scraped_at.desc())
        
        result = await db.execute(query)
        recent_articles = result.scalars().all()
        recent_urls = {article.url for article in recent_articles}
        
        # Scrape new articles
        scraped = await scraper.get_latest_articles(limit)
        new_articles = [a for a in scraped if a['url'] not in recent_urls]
        
        # Save new articles to database
        db_articles = []
        for article_data in new_articles:
            article = Content(
                type=ContentType.ARTICLE,
                url=article_data['url'],
                title=article_data['title'],
                content=article_data['content'],
                source='blockworks',
                scraped_at=datetime.fromisoformat(article_data['scraped_at'])
            )
            db.add(article)
            db_articles.append(article)
        
        await db.commit()
        
        # Start background task for summaries
        if db_articles:
            asyncio.create_task(generate_summaries_background(db_articles, model))
        
        return {
            "message": f"Successfully scraped {len(new_articles)} articles",
            "articles": [article.to_dict() for article in db_articles]
        }
    
    except Exception as e:
        logger.error(f"Error in scrape_articles: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def generate_summaries_background(articles: List[Content], model: Optional[str] = None):
    """Generate summaries in parallel with rate limiting"""
    # Limit concurrent summarizations to prevent overload
    semaphore = asyncio.Semaphore(2)  # Process 2 articles at a time
    
    async def generate_single_summary(article: Content):
        async with semaphore:
            try:
                # Notify start of summary generation
                await notify_clients(str(article.id), "Generating summary...", 0)
                
                # Generate summary
                summary = await llm.generate_summary(article.content, model)
                
                # Update database with summary
                async with AsyncSessionLocal() as db:
                    article.summary = summary
                    db.add(article)
                    await db.commit()
                
                # Notify completion
                await notify_clients(str(article.id), summary, 100)
                logger.info(f"Generated summary for article {article.id}")
                
            except Exception as e:
                logger.error(f"Error generating summary for article {article.id}: {str(e)}")
                error_msg = "Error generating summary"
                await notify_clients(str(article.id), error_msg, 100)
    
    # Create tasks for all articles but process them with semaphore limit
    tasks = [generate_single_summary(article) for article in articles]
    await asyncio.gather(*tasks)

@app.post("/generate-summary/{article_id}")
async def generate_single_article_summary(
    article_id: int, 
    model: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """Generate summary for a single article"""
    try:
        # Get article from database
        query = select(Content).where(Content.id == article_id)
        result = await db.execute(query)
        article = result.scalar_one_or_none()
        
        if not article:
            raise HTTPException(status_code=404, detail="Article not found")
        
        # Create background task for this single summary
        asyncio.create_task(generate_summaries_background([article], model))
        return {"message": "Summary generation started"}
    except Exception as e:
        logger.error(f"Error in generate_single_article_summary: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/articles")
async def get_articles(db: AsyncSession = Depends(get_db)):
    """Get all scraped articles"""
    try:
        query = select(Content).order_by(Content.scraped_at.desc())
        result = await db.execute(query)
        articles = result.scalars().all()
        return [article.to_dict() for article in articles]
    except Exception as e:
        logger.error(f"Error in get_articles: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query")
async def query_articles(
    query: Query,
    db: AsyncSession = Depends(get_db)
):
    """Query information about the articles"""
    try:
        # Get recent articles from database
        result = await db.execute(
            select(Content)
            .where(Content.type == ContentType.ARTICLE)
            .order_by(Content.scraped_at.desc())
        )
        articles = result.scalars().all()
        
        if not articles:
            raise HTTPException(status_code=404, detail="No articles available. Please scrape articles first.")
        
        # Convert to format expected by llm.query_articles
        article_dicts = [article.to_dict() for article in articles]
        response = await llm.query_articles(article_dicts, query.question, query.model)
        return {"response": response}
    except Exception as e:
        logger.error(f"Error in query_articles: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    """Get list of available models and their capabilities"""
    try:
        return await llm.list_available_models()
    except Exception as e:
        logger.error(f"Error in list_models: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

================
File: app/scraper.py
================
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BlockworksScraper:
    BASE_URL = "https://blockworks.co"
    
    async def fetch_page(self, url: str) -> str:
        logger.info(f"Fetching page: {url}")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers={'User-Agent': 'Mozilla/5.0'}) as response:
                    if response.status != 200:
                        logger.error(f"Error fetching {url}: Status {response.status}")
                        raise Exception(f"HTTP {response.status}")
                    return await response.text()
        except Exception as e:
            logger.error(f"Error fetching {url}: {str(e)}")
            raise

    async def extract_article_info(self, article_url: str) -> Dict:
        logger.info(f"Extracting article info from: {article_url}")
        try:
            html = await self.fetch_page(article_url)
            soup = BeautifulSoup(html, 'html.parser')
            
            # Updated selectors for title
            title = None
            title_candidates = [
                soup.find('h1'),  # Standard h1
                soup.find('meta', property='og:title'),  # Open Graph title
                soup.find('meta', {'name': 'title'})  # Meta title
            ]
            
            for candidate in title_candidates:
                if candidate:
                    if candidate.get('content'):  # For meta tags
                        title = candidate['content']
                        break
                    else:  # For h1 tags
                        title = candidate.text.strip()
                        break
            
            if not title:
                title = "No title found"
            
            # Updated content extraction
            content = ""
            # Try multiple potential content containers
            content_containers = [
                soup.find('article'),
                soup.find('div', class_='article-content'),
                soup.find('div', class_='post-content'),
                soup.find('main')
            ]
            
            for container in content_containers:
                if container:
                    # Get all text paragraphs
                    paragraphs = container.find_all(['p', 'h2', 'h3', 'h4'])
                    content = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])
                    if content:
                        break
            
            if not content:
                logger.warning(f"No article content found for {article_url}")
            
            return {
                "url": article_url,
                "title": title,
                "content": content,
                "scraped_at": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error extracting article info from {article_url}: {str(e)}")
            raise

    async def get_latest_articles(self, limit: int = 5) -> List[Dict]:
        logger.info(f"Getting latest {limit} articles")
        try:
            html = await self.fetch_page(f"{self.BASE_URL}/news")
            soup = BeautifulSoup(html, 'html.parser')
            
            # Updated article link finding
            article_links = []
            link_candidates = soup.find_all('a', href=True)
            
            for link in link_candidates:
                href = link['href']
                # Only include links that contain 'news' and aren't the main news page
                if '/news/' in href and href != '/news' and '/news/page/' not in href:
                    full_url = href if href.startswith('http') else self.BASE_URL + href
                    if full_url not in article_links:
                        article_links.append(full_url)
                        logger.info(f"Found article: {full_url}")
            
            if not article_links:
                logger.warning("No article links found on the page")
                return []
            
            article_links = article_links[:limit]
            logger.info(f"Processing {len(article_links)} articles")
            
            tasks = [self.extract_article_info(url) for url in article_links]
            articles = await asyncio.gather(*tasks, return_exceptions=True)
            
            valid_articles = [a for a in articles if isinstance(a, dict)]
            logger.info(f"Successfully processed {len(valid_articles)} articles")
            
            return valid_articles
        except Exception as e:
            logger.error(f"Error in get_latest_articles: {str(e)}")
            raise

================
File: migrations/versions/initial_migration.py
================
# migrations/versions/initial_migration.py
"""initial migration

Revision ID: initial
Revises: 
Create Date: 2024-12-10
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = 'initial'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    op.create_table('content',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('type', sa.Enum('ARTICLE', 'REDDIT', 'TWEET', name='contenttype'), nullable=False),
        sa.Column('url', sa.String(length=512), nullable=False),
        sa.Column('title', sa.String(length=512), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('summary', sa.Text(), nullable=True),
        sa.Column('source', sa.String(length=100), nullable=False),
        sa.Column('scraped_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('extra_data', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_type_scraped_at', 'content', ['type', 'scraped_at'])
    op.create_index(op.f('ix_content_scraped_at'), 'content', ['scraped_at'])

def downgrade():
    op.drop_index(op.f('ix_content_scraped_at'), table_name='content')
    op.drop_index('idx_type_scraped_at', table_name='content')
    op.drop_table('content')

================
File: migrations/env.py
================
# migrations/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
import asyncio
from sqlalchemy.ext.asyncio import AsyncEngine
import sys
from pathlib import Path

# Add the root project directory to Python path
root_path = Path(__file__).parents[1].resolve()
sys.path.append(str(root_path))

# Now we can import our app modules
from app.db.models import Base
from app.db.database import DATABASE_URL

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def run_migrations_offline():
    """Run migrations in 'offline' mode."""
    url = DATABASE_URL
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection):
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True
    )

    with context.begin_transaction():
        context.run_migrations()

async def run_migrations_online():
    """Run migrations in 'online' mode."""
    config_section = config.get_section(config.config_ini_section)
    url = DATABASE_URL
    config_section["sqlalchemy.url"] = url

    connectable = AsyncEngine(
        engine_from_config(
            config_section,
            prefix="sqlalchemy.",
            poolclass=pool.NullPool,
        )
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

if context.is_offline_mode():
    run_migrations_offline()
else:
    asyncio.run(run_migrations_online())

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

.DS_Store

================
File: alembic.ini
================
# alembic.ini

[alembic]
# path to migration scripts
script_location = migrations

# template used to generate migration files
file_template = %%(rev)s_%%(slug)s

# timezone to use when rendering the date
# within the migration file as well as the filename.
# string value is passed to dateutil.tz.gettz()
# leave blank for localtime
# timezone =

# max length of characters to apply to the
# "slug" field
truncate_slug_length = 40

sqlalchemy.url = postgresql+asyncpg://contentpulse:contentpulse@db:5432/contentpulse

[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

================
File: docker-compose.yml
================
version: '3.8'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - DATABASE_URL=postgresql+asyncpg://contentpulse:contentpulse@db:5432/contentpulse
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  db:
    image: postgres:16-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=contentpulse
      - POSTGRES_PASSWORD=contentpulse
      - POSTGRES_DB=contentpulse
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U contentpulse"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

volumes:
  ollama_data:
  postgres_data:

================
File: Dockerfile
================
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    netcat-openbsd \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Make entrypoint script executable
COPY entrypoint.sh .
RUN chmod +x entrypoint.sh

ENTRYPOINT ["./entrypoint.sh"]

================
File: entrypoint.sh
================
#!/bin/sh
# entrypoint.sh

# Install netcat first
echo "Installing netcat..."
apt-get update && apt-get install -y netcat-openbsd curl

# Wait for database
echo "Waiting for database..."
while ! nc -z db 5432; do
  sleep 0.1
done
echo "Database is ready!"

# Wait for Ollama to be ready
echo "Waiting for Ollama..."
while ! curl -s http://ollama:11434/api/health >/dev/null; do
  sleep 1
  echo "Waiting for Ollama to be ready..."
done
echo "Ollama is ready!"

# Pull the model if not already present
echo "Ensuring model is available..."
curl -X POST http://ollama:11434/api/pull -d '{"name": "llama3.2"}'
echo "Model setup complete"

# Run migrations
echo "Running database migrations..."
alembic upgrade head

# Start the application
echo "Starting application..."
exec uvicorn app.main:app --host 0.0.0.0 --port 8000

================
File: README.md
================
# Content Pulse

Real-time content aggregator and analyzer that distills insights from social media, news sources, and market data using LLMs. Currently supports crypto news analysis with planned expansion to stocks, geopolitics, and broader market sentiment tracking across Reddit, Twitter, and news platforms.

## Current Features
- Automated article scraping from crypto news sources (currently blockworks.co)
- Article summarization using Llama 3.2
- Natural language querying interface for article analysis
- In-memory data persistence
- Clean, responsive web interface

## Planned Features
- Reddit integration for crypto subreddit analysis
- Twitter sentiment analysis for crypto topics
- Expansion to traditional finance news sources
- Geopolitical news analysis
- Cross-platform sentiment correlation
- Historical data tracking and trend analysis

## Prerequisites
- Docker
- Docker Compose

## Quick Start
1. Start Ollama with Llama 3.2:
```bash
docker pull ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
docker exec -it ollama ollama pull llama3.2
```

2. Build and run the application:
```bash
docker compose up --build
```

3. Access the web interface at `http://localhost:8000`

## Architecture
- FastAPI backend for robust API performance
- Ollama for local LLM inference
- Async scraping and processing for improved performance
- Bootstrap frontend for responsive design
- Docker containerization for easy deployment

## Contributing
Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## License
MIT

## Project Status
🚧 Active Development - Core features implemented, actively expanding to new data sources and analysis capabilities.

================
File: requirements.txt
================
beautifulsoup4==4.12.2
requests==2.31.0
fastapi==0.104.1
uvicorn==0.24.0
python-dotenv==1.0.0
httpx==0.25.2
sqlalchemy==2.0.23
aiohttp==3.9.1
pydantic==2.5.2
websockets==11.0.3
asyncpg==0.29.0  # PostgreSQL async driver
alembic==1.13.1  # For database migrations
